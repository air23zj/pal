# PAL Launch Scripts

Convenient scripts to start and stop the PAL (Morning Brief AGI) application.

## ğŸš€ Launch Script (`launch.sh`)

Starts the complete PAL application stack:

1. **Checks LM Studio** - Verifies LM Studio is running and accessible
2. **Starts Backend** - Launches the FastAPI server on port 8000
3. **Starts Frontend** - Launches the Next.js dashboard on port 3000
4. **Opens Browser** - Automatically opens Chrome to the dashboard
5. **Tests Integration** - Verifies LLM connectivity

### Usage:
```bash
./scripts/launch.sh
```

### Requirements:
- LM Studio must be running with a model loaded
- Python dependencies installed (`pip install -r backend/requirements.txt`)
- Node.js dependencies installed (`cd frontend && npm install`)

## ğŸ›‘ Stop Script (`stop.sh`)

Stops all PAL processes and cleans up:

- Stops backend and frontend servers
- Kills processes by PID or name
- Clears occupied ports
- Removes temporary files

### Usage:
```bash
./scripts/stop.sh
```

## ğŸ“Š What the Launch Script Does:

### Pre-flight Checks:
- âœ… LM Studio connectivity test
- âœ… Process cleanup
- âœ… Dependency verification

### Service Startup:
- âœ… Backend server (FastAPI + LLM integration)
- âœ… Frontend server (Next.js dashboard)
- âœ… Health checks for both services
- âœ… LLM integration testing

### User Experience:
- âœ… Automatic browser opening
- âœ… Colored status output
- âœ… Process ID tracking
- âœ… Log file management

## ğŸ“ Generated Files:

When running `launch.sh`, these files are created:
- `logs/backend.log` - Backend server logs
- `logs/frontend.log` - Frontend server logs
- `pids.txt` - Process IDs for easy stopping
- `backend.pid` / `frontend.pid` - Individual process files

## ğŸ”§ Manual Control:

If you prefer manual control, you can also use the Makefile:

```bash
# Start everything
make dev

# Start backend only
make dev-backend

# Start frontend only
make dev-frontend

# Stop everything
make down
```

## ğŸ› Troubleshooting:

**"LM Studio is not running"**
- Open LM Studio application
- Load a model (e.g., llama-3.2-3b-instruct)
- Start the local server
- Run `./scripts/launch.sh` again

**"Backend failed to start"**
- Check logs: `tail -f logs/backend.log`
- Ensure dependencies: `cd backend && pip install -r requirements.txt`

**"Frontend failed to start"**
- Check logs: `tail -f logs/frontend.log`
- Ensure dependencies: `cd frontend && npm install`

## ğŸ“ Configuration:

The launch script uses the existing configuration:
- `.env` - Backend environment variables (project root)
- `frontend/.env.local` - Frontend configuration

Make sure LM Studio configuration is set in `.env`:
```
LLM_PROVIDER=openai
OPENAI_BASE_URL=http://localhost:1234/v1
LLM_MODEL=openai/gpt-oss-20b
```